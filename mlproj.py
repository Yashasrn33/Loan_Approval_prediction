# -*- coding: utf-8 -*-
"""MLPROJ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SEdMKuPjQVyMPJ-dgid5SG8hvyW0UIPv
"""

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
import matplotlib.pyplot as plt

"""# **Data collection and processing**"""

df = pd.read_csv('/content/train_u6lujuX_CVtuZ9i (1).csv')

df.head(5)

backup = df.copy()

df.shape

df.describe()

"""# **Numerical columns**
* ApplicantIncome
* CoapplicantIncome
* LoanAmount
* Loan_Amount_Term
* Credit_History
"""

df.isnull().sum()

df = df.dropna()

df.isnull().sum()

df.shape

df.replace({'Loan_Status':{'N':0,'Y':1}}, inplace=True)

df.head(5)

df['Dependents'].value_counts()

df.replace(to_replace='3+',value=4, inplace=True)

df['Dependents'].value_counts()

"""## **Data visualization**"""

# education and Loan Status
sns.countplot(x='Education',hue='Loan_Status', data=df)

# marital status and Loan status
sns.countplot(x='Married',hue='Loan_Status', data=df)

# Gender and Loan Status
sns.countplot(x='Gender',hue='Loan_Status', data=df)

# Dependents and Loan Status
sns.countplot(x='Dependents',hue='Loan_Status', data=df)

# Self_Employed and Loan Status
sns.countplot(x='Self_Employed',hue='Loan_Status', data=df)

# convert categorical columns to numerical values

df['Married'] = df['Married'].map({'Yes':1,'No':0})
df['Gender'] = df['Gender'].map({'Male':1,'Female':0})
df['Self_Employed'] = df['Self_Employed'].map({'Yes':1,'No':0})
df['Property_Area'] = df['Property_Area'].map({'Rural':0,'Semiurban':1,'Urban':2})
df['Education'] = df['Education'].map({'Graduate':1,'Not Graduate':0})

df.head()

# separating the data and label

X = df.drop(columns=['Loan_ID','Loan_Status'])
y = df.Loan_Status

X.head()

y.head()

# Visualize Correlation of All Features with Heatmap
plt.figure(figsize=(10,6))
sns.heatmap(round(df.corr(),2), annot=True, cmap="coolwarm")

"""## **Data Standardization**"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""## **Multi-colinearity (VIF)**"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = pd.DataFrame()

vif['Vif'] = [variance_inflation_factor(X_scaled,i) for i in range(X_scaled.shape[1])]
vif['features'] = X.columns
vif

"""## **Train Test Split**"""

X_train, X_test, y_train, y_test = train_test_split(X_scaled,y,test_size=0.2, stratify=y, random_state=2)

"""# **Training the Model:**

##**Support Vector Machine Model**
"""

classifier = svm.SVC(kernel='linear')
# training the support vector machine model
classifier.fit(X_train, y_train)

# accuracy score on training data
x_train_pred = classifier.predict(X_train)
print('Training Data accuracy: ',accuracy_score(x_train_pred, y_train))

# accuracy score on test data
x_test_pred = classifier.predict(X_test)
print('Training Data accuracy: ',accuracy_score(x_test_pred, y_test))



"""##**Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

y_pred = rf_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
precision = precision_score(y_test, y_pred)
print(f'precision: {precision:.2f}')

"""##**Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression()
# training the model
log_reg.fit(X_train, y_train)

log_y_train_pred = log_reg.predict(X_train)
accuracy_score(log_y_train_pred, y_train)

log_y_test_pred = log_reg.predict(X_test)
accuracy_score(log_y_test_pred, y_test)

"""##**Decision Tree classifier**"""

from sklearn.tree import DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train, y_train)

y_pred = dt_classifier.predict(X_test)
print(y_pred)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
precision = precision_score(y_test, y_pred)
print(f'precision: {precision:.2f}')

"""##**Linear Classifier**"""

linear_classifier = LogisticRegression(random_state=42)
# Fit the classifier to the training data
linear_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = linear_classifier.predict(X_test)
print(y_pred)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
precision = precision_score(y_test, y_pred)
print(f'precision: {precision:.2f}')

"""##**Naive Bayes classifier**"""

from sklearn.naive_bayes import GaussianNB
naive_bayes_classifier = GaussianNB()
naive_bayes_classifier.fit(X_train, y_train)

y_pred = naive_bayes_classifier.predict(X_test)
# Calculate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
precision = precision_score(y_test, y_pred)
print(f'precision: {precision:.2f}')

"""#**Regression Models**"""

# Import necessary libraries
import numpy as np
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Create a synthetic dataset
X, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression
linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)
y_pred_linear = linear_reg.predict(X_test)

# Decision Tree Regression
tree_reg = DecisionTreeRegressor(random_state=42)
tree_reg.fit(X_train, y_train)
y_pred_tree = tree_reg.predict(X_test)

# Random Forest Regression
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)
y_pred_rf = rf_reg.predict(X_test)

# Evaluate and print performance metrics
def evaluate_regression(y_true, y_pred, model_name):
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"{model_name} Metrics:")
    print(f"Mean Squared Error: {mse:.2f}")
    print(f"R-squared (R2) Score: {r2:.2f}\n")

evaluate_regression(y_test, y_pred_linear, "Linear Regression")
evaluate_regression(y_test, y_pred_tree, "Decision Tree Regression")
evaluate_regression(y_test, y_pred_rf, "Random Forest Regression")

# Plot the results
plt.figure(figsize=(12, 6))
plt.scatter(X_test, y_test, label="Actual")
plt.scatter(X_test, y_pred_linear, label="Linear Regression", marker='x', s=80)
plt.scatter(X_test, y_pred_tree, label="Decision Tree Regression", marker='o', s=80)
plt.scatter(X_test, y_pred_rf, label="Random Forest Regression", marker='^', s=80)
plt.xlabel("X")
plt.ylabel("y")
plt.title("Regression Models Comparison")
plt.legend()
plt.show()